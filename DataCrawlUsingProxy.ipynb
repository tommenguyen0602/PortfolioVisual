{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6ztmLQM9-QD",
        "outputId": "8feb4852-6609-4238-c892-d4dcbe52e812"
      },
      "outputs": [],
      "source": [
        "#!pip install beautifulsoup4 requests selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Collecting Signal IDs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Viewing IDs List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Signal Id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2.091000e+03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.756951e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.352849e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>8.527800e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.733086e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.838356e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.903918e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.943806e+06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Signal Id\n",
              "count  2.091000e+03\n",
              "mean   1.756951e+06\n",
              "std    2.352849e+05\n",
              "min    8.527800e+04\n",
              "25%    1.733086e+06\n",
              "50%    1.838356e+06\n",
              "75%    1.903918e+06\n",
              "max    1.943806e+06"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "IDs = pd.read_csv(\"signal_list.csv\")\n",
        "IDs.describe()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TLyREx7CD5PH"
      },
      "source": [
        "View Signal Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#signals = pd.read_csv(\"signal_details.csv\")\n",
        "#signals.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['14.139.242.7:80']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def load_proxies_from_csv(file_path):\n",
        "    proxies = []\n",
        "    with open(file_path, 'r') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        for row in csv_reader:\n",
        "            proxies.append(row)  # Assuming the proxy address is in the first column\n",
        "    return proxies\n",
        "\n",
        "def make_request(url, proxy):\n",
        "    try:\n",
        "        response = requests.get(url, proxies={'http': proxy, 'https': proxy})\n",
        "        if response.status_code == 200:\n",
        "            return response\n",
        "        elif response.status_code == 403:\n",
        "            print(f\"403 Forbidden error with proxy: {proxy}. Switching proxy...\")\n",
        "            return None\n",
        "        else:\n",
        "            print(f\"Error {response.status_code} occurred with proxy: {proxy}\")\n",
        "            return None\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"An error occurred with proxy: {proxy}. Error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def crawl_with_proxies(url, proxy_list):\n",
        "    for proxy in proxy_list:\n",
        "        print(f\"Trying proxy: {proxy}\")\n",
        "        response = make_request(url, proxy)\n",
        "        if response is not None:\n",
        "            # Process the response or perform desired actions\n",
        "            print(response)\n",
        "            break  # Exit the loop if a successful response is obtained\n",
        "\n",
        "# Usage example\n",
        "csv_file_path = 'proxies_list.csv'\n",
        "proxies = load_proxies_from_csv(csv_file_path)\n",
        "proxies[0]\n",
        "response = requests.get(\"https://www.mql5.com/en/signals/1491698?source=Site+Signals+MT5+Tile#!tab=stats\", proxies= {'http': proxies[0], 'https': proxy = proxies[0]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object, got 'list'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m url \u001b[39m=\u001b[39m base_url_singleID \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(row) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m?source=Unknown#!tab=stats\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[39m#Crawl each signal\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m response \u001b[39m=\u001b[39m make_request(url,proxies[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(response\u001b[39m.\u001b[39mstatus_code)\n\u001b[0;32m     12\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m403\u001b[39m:\n",
            "Cell \u001b[1;32mIn[7], line 11\u001b[0m, in \u001b[0;36mmake_request\u001b[1;34m(url, proxy)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_request\u001b[39m(url, proxy):\n\u001b[0;32m     10\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m         response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url, proxies\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m'\u001b[39;49m: proxy, \u001b[39m'\u001b[39;49m\u001b[39mhttps\u001b[39;49m\u001b[39m'\u001b[39;49m: proxy})\n\u001b[0;32m     12\u001b[0m         \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m     13\u001b[0m             \u001b[39mreturn\u001b[39;00m response\n",
            "File \u001b[1;32mc:\\Users\\tomme\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\tomme\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\tomme\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[1;32mc:\\Users\\tomme\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
            "File \u001b[1;32mc:\\Users\\tomme\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:455\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Sends PreparedRequest object. Returns Response object.\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \n\u001b[0;32m    440\u001b[0m \u001b[39m:param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 455\u001b[0m     conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_connection(request\u001b[39m.\u001b[39;49murl, proxies)\n\u001b[0;32m    456\u001b[0m \u001b[39mexcept\u001b[39;00m LocationValueError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    457\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidURL(e, request\u001b[39m=\u001b[39mrequest)\n",
            "File \u001b[1;32mc:\\Users\\tomme\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:344\u001b[0m, in \u001b[0;36mHTTPAdapter.get_connection\u001b[1;34m(self, url, proxies)\u001b[0m\n\u001b[0;32m    341\u001b[0m proxy \u001b[39m=\u001b[39m select_proxy(url, proxies)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m proxy:\n\u001b[1;32m--> 344\u001b[0m     proxy \u001b[39m=\u001b[39m prepend_scheme_if_needed(proxy, \u001b[39m\"\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    345\u001b[0m     proxy_url \u001b[39m=\u001b[39m parse_url(proxy)\n\u001b[0;32m    346\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m proxy_url\u001b[39m.\u001b[39mhost:\n",
            "File \u001b[1;32mc:\\Users\\tomme\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\utils.py:993\u001b[0m, in \u001b[0;36mprepend_scheme_if_needed\u001b[1;34m(url, new_scheme)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepend_scheme_if_needed\u001b[39m(url, new_scheme):\n\u001b[0;32m    988\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Given a URL that may or may not have a scheme, prepend the given scheme.\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[39m    Does not replace a present scheme with the one provided as an argument.\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \n\u001b[0;32m    991\u001b[0m \u001b[39m    :rtype: str\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 993\u001b[0m     parsed \u001b[39m=\u001b[39m parse_url(url)\n\u001b[0;32m    994\u001b[0m     scheme, auth, host, port, path, query, fragment \u001b[39m=\u001b[39m parsed\n\u001b[0;32m    996\u001b[0m     \u001b[39m# A defect in urlparse determines that there isn't a netloc present in some\u001b[39;00m\n\u001b[0;32m    997\u001b[0m     \u001b[39m# urls. We previously assumed parsing was overly cautious, and swapped the\u001b[39;00m\n\u001b[0;32m    998\u001b[0m     \u001b[39m# netloc and path. Due to a lack of tests on the original defect, this is\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[39m# maintained with parse_url for backwards compatibility.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\tomme\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\url.py:360\u001b[0m, in \u001b[0;36mparse_url\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[39mreturn\u001b[39;00m Url()\n\u001b[0;32m    359\u001b[0m source_url \u001b[39m=\u001b[39m url\n\u001b[1;32m--> 360\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SCHEME_RE\u001b[39m.\u001b[39;49msearch(url):\n\u001b[0;32m    361\u001b[0m     url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m//\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m url\n\u001b[0;32m    363\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'list'"
          ]
        }
      ],
      "source": [
        "base_url_singleID = 'https://www.mql5.com/en/signals/'\n",
        "with open('signal_details.csv', 'w', newline='') as signal_detailsFile:\n",
        "  writer = csv.writer(signal_detailsFile)\n",
        "  writer.writerow(['Signal Id','Trades','ProfitTrades','LossTrades','BestTrades','WorstTrades','GrossProfit','GrossLoss','SharpeRatio','RecoveryFactor','ProfitFactor','ExpectedPayoff','AlgoTrading'])\n",
        "#load the csv containing Signal IDs\n",
        "for proxy in proxies:\n",
        "  start_point = 782\n",
        "  for row in IDs.iloc[start_point:]['Signal Id']:\n",
        "    indexing = start_point + 1\n",
        "    url = base_url_singleID + str(row) + \"?source=Unknown#!tab=stats\"\n",
        "    #Crawl each signal\n",
        "    response = make_request(url,proxies[0])\n",
        "    print(response.status_code)\n",
        "    if response.status_code == 403:\n",
        "      start_point = indexing\n",
        "      break\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    \n",
        "    values = soup.find_all('div', class_='s-data-columns__value')\n",
        "    if len(values) == 0:\n",
        "        continue\n",
        "    # Loop over each product and extract the data\n",
        "    signalTrades = values[0].text \n",
        "\n",
        "\n",
        "  # Get total number of profit trades\n",
        "    s = values[1].text\n",
        "    number_regex = r\"([\\d\\s]+)\\s*\\(.*\\)\"\n",
        "    match = re.search(number_regex, s)\n",
        "    if match:\n",
        "        number = match.group(1)\n",
        "        # Remove spaces and normalize the number\n",
        "        normalized_number = float(number.replace(\" \", \"\").replace(\",\", \"\"))\n",
        "    signalProfitTrades = normalized_number\n",
        "\n",
        "    # Get total number of loss trades\n",
        "\n",
        "    s = values[2].text\n",
        "    number_regex = r\"([\\d\\s]+)\\s*\\(.*\\)\"\n",
        "    match = re.search(number_regex, s)\n",
        "    if match:\n",
        "        number = match.group(1)\n",
        "        # Remove spaces and normalize the number\n",
        "        normalized_number = float(number.replace(\" \", \"\").replace(\",\", \"\"))\n",
        "    signalLossTrades = normalized_number\n",
        "\n",
        "    signalBestTrade = values[3].find('i').text\n",
        "    signalWorstTrade = values[4].find('i').text\n",
        "\n",
        "    signalGrossProfit = values[5].find('i').text\n",
        "    signalGrossLoss = values[6].find('i').text\n",
        "\n",
        "\n",
        "\n",
        "    signalSharpeRatio = values[9].text\n",
        "\n",
        "    signalRecoveryFactor = values[15].text\n",
        "\n",
        "\n",
        "    signalProfitFactor = values[18].text\n",
        "\n",
        "    signalExpectedPayoff = values[19].find('i').text\n",
        "\n",
        "    # Find the <svg> element\n",
        "\n",
        "    if \"Algo trading\" in values[25].parent.text:\n",
        "      pattern = r'%'\n",
        "      signalAlgoTrading = re.sub(pattern, '', values[25].text)\n",
        "    else :\n",
        "      pattern = r'%'\n",
        "      signalAlgoTrading = re.sub(pattern, '', values[26].text)\n",
        "\n",
        "    values2 = soup.find_all('div', class_='s-list-info__value')\n",
        "    signalGrowth = re.sub(pattern, '', values2[0].text) #percentage\n",
        "    with open('signal_details.csv', 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([row,signalTrades,signalProfitTrades,signalLossTrades,signalBestTrade,signalWorstTrade,signalGrossProfit,signalGrossLoss,signalSharpeRatio,signalRecoveryFactor,signalProfitFactor,signalExpectedPayoff,signalAlgoTrading])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
